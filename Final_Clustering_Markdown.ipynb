        "**Dataset:** Online Retail \u2013 UCI Machine Learning Repository  \n",
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations on the raw dataset\n",
        "- The pull from UCI contains **541,909 transaction rows across 8 columns**.\n",
        "- **CustomerID is missing on 135,080 rows and Description on 1,454 rows**, so customer-level analysis needs those records removed.\n",
        "- Descriptive stats show extreme negative values (Quantity as low as -80,995 and UnitPrice down to -11,062.06) alongside very large maxima, confirming the presence of returns or data errors that must be filtered before modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EDA takeaways\n",
        "- Univariate plots for Quantity and UnitPrice are heavily right-skewed with long positive tails, reflecting a small set of very large orders/prices and many low-value transactions.\n",
        "- These skews reinforce the need for cleaning negative values and later scaling before clustering.\n"
      ]
    },
        "   \u2013 These rows cannot be used for customer-level behaviour analysis.  \n",
        "   \u2013 Negative quantities usually represent returns; we focus on positive purchase behaviour.  \n",
        "   \u2013 Zero price may indicate errors or free items not relevant for revenue-based clustering.  \n",
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data cleaning impact\n",
        "- Removing rows without CustomerID/Description reduced the dataset to **406,829** rows, and filtering invalid Quantity/UnitPrice brought it to **397,884** rows.\n",
        "- The majority of dropped records stem from missing customer identifiers and return-like transactions, aligning with the issues seen in the initial descriptive stats.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Customer-level feature quality\n",
        "- Aggregation yields **4,338 customers**, with **4,267 usable** after removing missing average inter-purchase times.\n",
        "- Feature completeness is strong across Recency, Frequency, Monetary, and variety metrics, so subsequent scaling and PCA operate on a mostly intact customer base.\n"
      ]
    },
        "plt.title(\"Correlation Heatmap \u2013 Customer-Level Features\")\n",
        "## PCA \u2013 DIMENSIONALITY REDUCTION"
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA observations\n",
        "- Selecting components to explain 90% of the variance keeps **4 principal components**, reducing dimensionality while retaining most signal.\n",
        "- The first two PCs provide a reasonable 2D view but still show overlap between customers, implying clustering will capture nuanced structure rather than sharply separated groups.\n"
      ]
    },
        "plt.title(\"PCA Projection \u2013 First Two Components\")\n",
        "plt.title(\"K-Means \u2013 Elbow Method (PCA Space)\")\n",
        "plt.title(\"K-Means \u2013 Silhouette Score by k\")\n",
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-Means results\n",
        "- Silhouette scores rise through \\(k=5\\) (\u22480.44) and dip afterwards, supporting the choice of **5 clusters** as a balance between cohesion and separation.\n",
        "- The PCA scatter shows moderate separation with some overlap, so clusters should be interpreted as relative customer tiers rather than rigid boundaries.\n"
      ]
    },
        "plt.title(\"Customer Clusters \u2013 K-Means (Behavioural Only)\")\n",
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DBSCAN results\n",
        "- Using **eps=1.5, min_samples=5** finds two dense clusters with a small noise set and a high silhouette (\u22480.81), indicating well-separated dense regions in PCA space.\n",
        "- Stricter settings (eps=1.0, min_samples=10) collapse into a single cluster with many noise points, so the chosen parameters balance separation with coverage.\n"
      ]
    },
        "plt.title(f\"Customer Clusters \u2013 DBSCAN (Behavioural Only, eps={chosen_eps}, min_samples={chosen_min_samples})\")\n",
        "plt.title(f\"Customer Clusters \u2013 DBSCAN (Behavioural Only, eps={chosen_eps}, min_samples={chosen_min_samples})\")\n",
        "## MODEL COMPARISON \u2013 KMEANS VS DBSCAN"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insights and Recommendations\n",
        "- **Customer activity skews:** Transactions are dominated by low-quantity, low-price orders with a long tail of large purchases; cleaning negative values and scaling were essential to stabilise clustering.\n",
        "- **Feature readiness:** After cleaning, 4,267 customers retained robust behavioural signals (recency, frequency, monetary value, variety), enabling reliable distance calculations without heavy imputation.\n",
        "- **Model preference:** K-Means with five clusters yields balanced segments (silhouette \u22480.44), while DBSCAN uncovers two very dense groups with strong separation but treats more edge cases as noise.\n",
        "- **Engagement strategy:** Use the five K-Means segments to tailor messaging (e.g., nurture low-frequency buyers, reward high-monetary loyalists) and supplement with DBSCAN\u2019s dense-cluster flags to target the most consistently active shoppers for VIP or replenishment campaigns.\n",
        "- **Next steps:** Validate clusters against downstream KPIs (repeat purchase rate, churn), trial promotions per segment, and monitor drift; revisit PCA/clustering thresholds quarterly as new data arrives.\n"
      ]
